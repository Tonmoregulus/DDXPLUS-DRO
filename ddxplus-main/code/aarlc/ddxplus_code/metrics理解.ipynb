{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61bdf225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def write_json(data, fp):\n",
    "    with open(fp, \"w\") as outfile:\n",
    "        json.dump(data, outfile, indent=4)\n",
    "\n",
    "def get_average_state_from_percent(data, percent=0.0, end_percent=1.0, normalize=True):\n",
    "    \"\"\"Get the average stat from a data sequence starting at a given percent.\n",
    "    Parameters\n",
    "    ----------\n",
    "    data:  sequence\n",
    "        the provided data.\n",
    "    percent: float, list, tuple\n",
    "        the provided percentage. Default: 0.0.\n",
    "    end_percent: float, list, tuple\n",
    "        the provided end percentage. Default: 1.0.\n",
    "    normalize: boolean\n",
    "        whether to normalize\n",
    "    Returns\n",
    "    -------\n",
    "    result: float, list\n",
    "        the computed average stat.\n",
    "    \"\"\"\n",
    "    assert not (percent is None and end_percent is None)\n",
    "    if percent is None:\n",
    "        percent = end_percent\n",
    "    elif end_percent is None:\n",
    "        end_percent = percent\n",
    "    assert isinstance(percent, (int, float, list, tuple))\n",
    "    assert isinstance(end_percent, (int, float, list, tuple))\n",
    "    extract_flag = False\n",
    "    if isinstance(percent, (int, float)) and isinstance(end_percent, (int, float)):\n",
    "        percent = [percent]\n",
    "        end_percent = [end_percent]\n",
    "        extract_flag = True\n",
    "    elif isinstance(percent, (int, float)):\n",
    "        percent = [percent] * len(end_percent)\n",
    "    elif isinstance(end_percent, (int, float)):\n",
    "        end_percent = [end_percent] * len(percent)\n",
    "    assert len(percent) == len(end_percent)\n",
    "    length = len(data)\n",
    "    result = []\n",
    "    for b, e in zip(percent, end_percent):\n",
    "        assert b >= 0.0 and b <= 1.0\n",
    "        assert e >= 0.0 and e <= 1.0\n",
    "        assert e >= b\n",
    "        i = int(length * b)\n",
    "        j = int(length * e)\n",
    "        j = j + 1 if j == i else j\n",
    "        j = min(length, j)\n",
    "        i = i - 1 if i == length else i\n",
    "        n = j - i\n",
    "        r = sum(data[i:j])\n",
    "        r = r / max(1, n) if normalize else r\n",
    "        result.append(r)\n",
    "    if len(result) == 1 and extract_flag:\n",
    "        result = result[0]\n",
    "    return result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8841bc",
   "metadata": {},
   "source": [
    "提供的代码片段包括两个实用函数： write_json(data, fp)：此函数将给定数据写入位于指定文件路径 (fp) 的 JSON 文件。数据以 4 个空格的缩进级别写入。 get_average_state_from_percent(data, percent=0.0, end_percent=1.0, normalize=True)：此函数计算从给定百分比开始到结束百分比结束的数据序列的平均统计数据。参数如下： 数据：数据的输入序列。 percent：数据序列的起始百分比。它可以是单个值或值的列表/元组。默认值为 0.0。 end_percent：数据序列的结束百分比。它可以是单个值或值的列表/元组。默认值为 1.0。 normalize：一个布尔值，指示是否对计算的平均值进行归一化。如果为 True，则平均值除以所选数据范围内的元素数。默认为真。 该函数处理 percent 和 end_percent 参数的不同情况。它支持单个值或具有不同组合的多个值。它计算每个指定范围的平均统计数据，并根据输入将结果作为单个值或列表返回。 请注意，该函数假定数据是一个序列（例如，列表、numpy 数组）并执行切片操作以提取所需的范围。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d9342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_severity_stats(preds, gt_diff, severity_mask, diff_proba_threshold):\n",
    "    if severity_mask is None:\n",
    "        severity_mask = [0] * preds.shape[-1]\n",
    "    if len(preds.shape) > len(gt_diff.shape):\n",
    "        tmp_shape = ([1] * (len(preds.shape) - len(gt_diff.shape))) + list(gt_diff.shape)\n",
    "        gt_diff = gt_diff.reshape(tmp_shape)\n",
    "    sev_shape = ([1] * (len(gt_diff.shape) - 1)) + [-1]\n",
    "    severity_mask= np.array(severity_mask).reshape(sev_shape)\n",
    "    gt_severity_mask = ((gt_diff > diff_proba_threshold) * severity_mask).astype(bool)\n",
    "    pr_severity_mask = ((preds > diff_proba_threshold) * severity_mask).astype(bool)\n",
    "    # mask of severe patho in pred not in gt\n",
    "    pr_gt_nonshared_severity_mask = (np.logical_not(gt_severity_mask) * pr_severity_mask).astype(bool)\n",
    "    # mask of severe patho in gt not in pred\n",
    "    gt_pr_nonshared_severity_mask = (np.logical_not(pr_severity_mask) * gt_severity_mask).astype(bool)\n",
    "    nb_out = pr_gt_nonshared_severity_mask.sum(-1)\n",
    "    nb_in = gt_pr_nonshared_severity_mask.sum(-1)\n",
    "    gt_nbSev = gt_severity_mask.astype(int).sum(-1)\n",
    "    max_sev = severity_mask.sum()\n",
    "    pred_no_gt = (max_sev - gt_nbSev - nb_out) / np.maximum(1, max_sev - gt_nbSev)\n",
    "    gt_no_pred = (gt_nbSev - nb_in) / np.maximum(1, gt_nbSev)\n",
    "    gt_pred_f1 = compute_f1(pred_no_gt, gt_no_pred)\n",
    "    return pred_no_gt, gt_no_pred, gt_pred_f1\n",
    "    \n",
    "    \n",
    "\n",
    "def kl_trajectory_auc(kl_explore, kl_confirm):\n",
    "    kl_explore = np.array(kl_explore)\n",
    "    kl_confirm = np.array(kl_confirm)\n",
    "    result = np.trapz(kl_explore, kl_confirm, axis=-1)\n",
    "    return result\n",
    "\n",
    "def kl_confirm_score(probas, dist, c=1):\n",
    "    entropy = -np.sum(dist * np.log(dist + 1e-10), axis=-1)\n",
    "    if len(probas.shape) > len(dist.shape):\n",
    "        tmp_shape = ([1] * (len(probas.shape) - len(dist.shape))) + list(dist.shape)\n",
    "        dist = dist.reshape(tmp_shape)\n",
    "    cross_entropy = -np.sum(dist * np.log(probas + 1e-10), axis=-1)\n",
    "    kl_val = np.maximum(0, cross_entropy - entropy)\n",
    "    kl_val = np.exp(-c * kl_val)\n",
    "    return kl_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea76768",
   "metadata": {},
   "source": [
    "提供的代码片段包括三个函数： compute_severity_stats(preds, gt_diff, severity_mask, diff_proba_threshold)：此函数根据预测 (preds)、地面实况差异 (gt_diff)、严重性掩码 (severity_mask) 和微分概率阈值 (diff_proba_threshold) 计算严重性统计数据。参数如下： preds：预测的差异。 gt_diff：地面实况差异。 severity_mask：表示每种病理学严重程度的掩码。它用于根据病症的严重程度过滤掉病症。如果没有，则使用全零的默认严重性掩码。 diff_proba_threshold：用于确定病理是否严重的微分概率阈值。 该函数执行各种计算以计算不同的严重性统计数据，包括预测中不存在于基本事实中的严重病理学数量、基本事实中不存在于预测中的严重病理学数量以及 F1 分数与严重病理学预测但不存在于基本事实中，以及严重病理学在基本事实中但不存在于预测中的数量。该函数将计算的统计信息返回为 pred_no_gt、gt_no_pred 和 gt_pred_f1。 kl_trajectory_auc(kl_explore, kl_confirm)：此函数计算由 kl_explore 和 kl_confirm 定义的轨迹的曲线下面积 (AUC)。参数如下： kl_explore：沿轨迹的探索值（例如，KL 散度）。 kl_confirm：沿轨迹的确认值（例如，KL 散度）。 该函数使用梯形规则来近似曲线下的面积并将结果作为结果返回。 kl_confirm_score(probas, dist, c=1)：此函数根据预测概率 (probas) 和真实分布 (dist) 计算确认分数。参数 c 是一个比例因子，用于控制确认分数的影响。该函数计算真实分布的熵、预测概率与真实分布之间的交叉熵，然后将确认分数计算为交叉熵与熵之差的指数乘以比例因子 c。该函数将确认分数作为 kl_val 返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb6fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_explore_score(probas, first_proba=None, c=1):\n",
    "    dist = probas[0] if first_proba is None else first_proba\n",
    "    entropy = -np.sum(dist * np.log(dist + 1e-10), axis=-1)\n",
    "    if len(probas.shape) > len(dist.shape):\n",
    "        tmp_shape = ([1] * (len(probas.shape) - len(dist.shape))) + list(dist.shape)\n",
    "        dist = dist.reshape(tmp_shape)\n",
    "    cross_entropy = -np.sum(dist * np.log(probas + 1e-10), axis=-1)\n",
    "    kl_val = np.maximum(0, cross_entropy - entropy)\n",
    "    kl_val = np.exp(-c * kl_val)\n",
    "    kl_val = 1.0 - kl_val\n",
    "    return kl_val\n",
    "    \n",
    "def kl_trajectory_score(kl_explore, kl_confirm, alphas=None):\n",
    "    kl_explore = np.array(kl_explore)\n",
    "    kl_confirm = np.array(kl_confirm)\n",
    "    if alphas is None:\n",
    "        if kl_explore.shape[-1] > 1:\n",
    "            alphas = np.arange(start=kl_explore.shape[-1] - 1, stop=-1, step=-1)\n",
    "            alphas = alphas / (kl_explore.shape[-1] - 1)\n",
    "            if len(kl_explore.shape) > len(alphas.shape):\n",
    "                tmp_shape = ([1] * (len(kl_explore.shape) - len(alphas.shape))) + list(alphas.shape)\n",
    "                alphas = alphas.reshape(tmp_shape)\n",
    "        else:\n",
    "            alphas = 0.5\n",
    "    score = alphas * kl_explore + (1 - alphas) * kl_confirm\n",
    "    return score\n",
    "\n",
    "def compute_f1(p, r):\n",
    "    if isinstance(p, (list, tuple)):\n",
    "        p = np.array(p)\n",
    "    if isinstance(r, (list, tuple)):\n",
    "        r = np.array(r)\n",
    "    denom = p + r\n",
    "    return (2 * p * r) / (denom + 1e-10)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ec9658",
   "metadata": {},
   "source": [
    "提供的代码片段包括三个附加功能： kl_explore_score(probas, first_proba=None, c=1)：此函数根据预测概率 (probas) 计算探索分数。参数 first_proba 是可选参数，表示初始概率分布。参数 c 是一个比例因子，用于控制探索分数的影响。该函数计算初始分布（或第一概率分布）的熵，预测概率与初始分布之间的交叉熵，然后计算探索得分作为交叉熵与熵之间差异的指数，乘以比例因子 c。该函数将探索分数作为 kl_val 返回。通过从 1 中减去 exploration score 得到最终分数，进一步转换 exploration score。 kl_trajectory_score(kl_explore, kl_confirm, alphas=None)：此函数根据探索分数 (kl_explore) 和确认分数 (kl_confirm) 计算轨迹分数。参数 alphas 是一个可选参数，用于指定在计算轨迹分数时分配给探索分数和确认分数的权重。如果未提供 alphas，则使用默认的线性加权方案。该函数使用指定的权重计算探索分数和确认分数的加权和，并将轨迹分数作为分数返回。 compute_f1(p, r)：此函数根据精度 (p) 和召回率 (r) 计算 F1 分数。精度和召回值可以列表、元组或 numpy 数组的形式提供。该函数使用公式 (2 * p * r) / (p + r + 1e-10) 计算 F1 分数并返回结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127783d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(gt_differential, disease, final_diags, all_diags, valid_timesteps, present_evidences, inquired_evidences, symptom_mask, atcd_mask, severity_mask, tres=0.01):\n",
    "        all_indices = list(range(disease.shape[0]))\n",
    "        top_ranked = np.argsort(final_diags, axis=-1)\n",
    "        top_ranked = top_ranked[:, ::-1]\n",
    "        gt_diff_top_ranked = np.argsort(gt_differential, axis=-1)\n",
    "        gt_diff_top_ranked = gt_diff_top_ranked[:, ::-1]\n",
    "        all_len = np.sum(valid_timesteps, axis=-1) + 1\n",
    "        \n",
    "        result = {}\n",
    "        result[\"IL\"] = np.mean(all_len)\n",
    "        result[\"GTPA\"] = np.mean(final_diags[all_indices, disease] > tres)\n",
    "        result[\"GTPA@1\"] = np.mean(np.logical_and(disease == top_ranked[:, 0], final_diags[all_indices, disease] > tres))\n",
    "        result[\"GTPA@3\"] = np.mean(np.logical_and(np.any(disease.reshape(-1, 1) == top_ranked[:, 0:3], axis=-1), final_diags[all_indices, disease] > tres))\n",
    "        result[\"GTPA@5\"] = np.mean(np.logical_and(np.any(disease.reshape(-1, 1) == top_ranked[:, 0:5], axis=-1), final_diags[all_indices, disease] > tres))\n",
    "\n",
    "        gt_diff_mask = (gt_differential > tres)\n",
    "        pred_diff_mask = (final_diags > tres)\n",
    "\n",
    "        ddr = np.sum(np.logical_and(gt_diff_mask, pred_diff_mask), axis=-1) / np.maximum(1, np.sum(gt_diff_mask, axis=-1))\n",
    "        ddp = np.sum(np.logical_and(gt_diff_mask, pred_diff_mask), axis=-1) / np.maximum(1, np.sum(pred_diff_mask, axis=-1))\n",
    "        ddf1 = compute_f1(ddp, ddr)\n",
    "\n",
    "        result[f\"DDR\"] = np.mean(ddr)\n",
    "        result[f\"DDP\"] = np.mean(ddp)\n",
    "        result[f\"DDF1\"] = np.mean(ddf1)\n",
    "        for k in [1, 3, 5]:\n",
    "            tmp_gt_k = np.zeros_like(gt_diff_mask).astype(bool)\n",
    "            np.put_along_axis(tmp_gt_k, gt_diff_top_ranked[:, 0:k], True, 1)\n",
    "            tmp_gt_k = np.logical_and(gt_diff_mask, tmp_gt_k)\n",
    "\n",
    "            tmp_pred_k = np.zeros_like(pred_diff_mask).astype(bool)\n",
    "            np.put_along_axis(tmp_pred_k, top_ranked[:, 0:k], True, 1)\n",
    "            tmp_pred_k = np.logical_and(pred_diff_mask, tmp_pred_k)\n",
    "\n",
    "            tmp_ddr = np.sum(np.logical_and(tmp_gt_k, tmp_pred_k), axis=-1) / np.maximum(1, np.sum(tmp_gt_k, axis=-1))\n",
    "            tmp_ddp = np.sum(np.logical_and(tmp_gt_k, tmp_pred_k), axis=-1) / np.maximum(1, np.sum(tmp_pred_k, axis=-1))\n",
    "            tmp_ddf1 = compute_f1(tmp_ddp, tmp_ddr)\n",
    "            result[f\"DDR@{k}\"] = np.mean(tmp_ddr)\n",
    "            result[f\"DDP@{k}\"] = np.mean(tmp_ddp)\n",
    "            result[f\"DDF1@{k}\"] = np.mean(tmp_ddf1)\n",
    "\n",
    "        dsp, dsr, dsf1 = compute_severity_stats(final_diags, gt_differential, severity_mask, tres)\n",
    "        result[\"DSP\"] = np.mean(dsp)\n",
    "        result[\"DSR\"] = np.mean(dsr)\n",
    "        result[\"DSF1\"] = np.mean(dsf1)\n",
    "\n",
    "        pos_evi = np.logical_and(present_evidences, inquired_evidences)\n",
    "        per = np.sum(pos_evi, axis=-1) / np.maximum(1, np.sum(present_evidences, axis=-1))\n",
    "        pep = np.sum(pos_evi, axis=-1) / np.maximum(1, np.sum(inquired_evidences, axis=-1))\n",
    "        pef1 = compute_f1(pep, per)\n",
    "\n",
    "        result[\"PER\"] = np.mean(per)\n",
    "        result[\"PEP\"] = np.mean(pep)\n",
    "        result[\"PEF1\"] = np.mean(pef1)\n",
    "\n",
    "        present_symptoms = np.logical_and(present_evidences, symptom_mask.reshape(1, -1))\n",
    "        inquired_symptoms = np.logical_and(inquired_evidences, symptom_mask.reshape(1, -1))\n",
    "        pos_symp = np.logical_and(present_symptoms, inquired_symptoms)\n",
    "        psr = np.sum(pos_symp, axis=-1) / np.maximum(1, np.sum(present_symptoms, axis=-1))\n",
    "        psp = np.sum(pos_symp, axis=-1) / np.maximum(1, np.sum(inquired_symptoms, axis=-1))\n",
    "        psf1 = compute_f1(psp, psr)\n",
    "\n",
    "        result[\"PSR\"] = np.mean(psr)\n",
    "        result[\"PSP\"] = np.mean(psp)\n",
    "        result[\"PSF1\"] = np.mean(psf1)\n",
    "\n",
    "        present_atcds = np.logical_and(present_evidences, atcd_mask.reshape(1, -1))\n",
    "        inquired_atcds = np.logical_and(inquired_evidences, atcd_mask.reshape(1, -1))\n",
    "        pos_atcd = np.logical_and(present_atcds, inquired_atcds)\n",
    "        par = np.sum(pos_atcd, axis=-1) / np.maximum(1, np.sum(present_atcds, axis=-1))\n",
    "        pap = np.sum(pos_atcd, axis=-1) / np.maximum(1, np.sum(inquired_atcds, axis=-1))\n",
    "        paf1 = compute_f1(pap, par)\n",
    "\n",
    "        result[\"PAR\"] = np.mean(par)\n",
    "        result[\"PAP\"] = np.mean(pap)\n",
    "        result[\"PAF1\"] = np.mean(paf1)\n",
    "\n",
    "\n",
    "        tmp_shape = list(gt_differential.shape)\n",
    "        tmp_shape = tmp_shape[0:1] + [1] + tmp_shape[1:]\n",
    "        gt_diff_proba = gt_differential.reshape(tmp_shape)\n",
    "        confirm_score = kl_confirm_score(all_diags, gt_diff_proba)\n",
    "        explore_score = kl_explore_score(all_diags, first_proba=all_diags[:, 0:1])\n",
    "        succesive_explore_score = kl_explore_score(all_diags[:, 1:], first_proba=all_diags[:, 0:-1])\n",
    "\n",
    "        pred_no_gt, gt_no_pred, gt_pred_f1 = compute_severity_stats(all_diags, gt_diff_proba, severity_mask, tres)\n",
    "\n",
    "        p = list(range(0, 105, 5))\n",
    "        # p_idx = {v: i for i, v in enumerate(p)}\n",
    "        p = [v / 100.0 for v in p]\n",
    "        \n",
    "        t_explore_score = 0\n",
    "        t_succesive_explore_score = 0\n",
    "        t_confirm_score = 0\n",
    "        t_kl_trajectory_values = 0\n",
    "        t_pred_no_gt = 0\n",
    "        t_gt_no_pred = 0\n",
    "        t_gt_pred_f1 = 0\n",
    "        kl_trajectory_values_sum = 0\n",
    "        kl_trajectory_auc_sum = 0\n",
    "        tvd_sum = 0\n",
    "        for i in range(len(all_len)):\n",
    "            if all_len[i] == 0:\n",
    "                continue\n",
    "            mini_prob = np.amin(all_diags[i, 0:all_len[i]], axis=0)\n",
    "            maxi_prob = np.amax(all_diags[i, 0:all_len[i]], axis=0)    \n",
    "            tvd_sum += np.absolute(maxi_prob - mini_prob).mean()\n",
    "            kl_trajectory_values = kl_trajectory_score(explore_score[i, 0:all_len[i]], confirm_score[i, 0:all_len[i]])\n",
    "            kl_trajectory_values_sum += np.mean(kl_trajectory_values)\n",
    "            kl_trajectory_auc_sum += kl_trajectory_auc(explore_score[i, 0:all_len[i]], confirm_score[i, 0:all_len[i]])\n",
    "            t_explore_score = t_explore_score + np.array(get_average_state_from_percent(explore_score[i, 0:all_len[i]], percent=p, end_percent=None))\n",
    "            t_succesive_explore_score = t_succesive_explore_score + np.array(get_average_state_from_percent(succesive_explore_score[i, 0:all_len[i]-1], percent=p, end_percent=None))\n",
    "            t_confirm_score = t_confirm_score + np.array(get_average_state_from_percent(confirm_score[i, 0:all_len[i]], percent=p, end_percent=None))\n",
    "            t_kl_trajectory_values = t_kl_trajectory_values + np.array(get_average_state_from_percent(kl_trajectory_values, percent=p, end_percent=None))\n",
    "            t_pred_no_gt = t_pred_no_gt + np.array(get_average_state_from_percent(pred_no_gt[i, 0:all_len[i]], percent=p, end_percent=None))\n",
    "            t_gt_no_pred = t_gt_no_pred + np.array(get_average_state_from_percent(gt_no_pred[i, 0:all_len[i]], percent=p, end_percent=None))\n",
    "            t_gt_pred_f1 = t_gt_pred_f1 + np.array(get_average_state_from_percent(gt_pred_f1[i, 0:all_len[i]], percent=p, end_percent=None))\n",
    "        \n",
    "        tvd_sum /= max(1, len(all_len))\n",
    "        kl_trajectory_values_sum /= max(1, len(all_len))\n",
    "        kl_trajectory_auc_sum /= max(1, len(all_len))\n",
    "        t_explore_score /= max(1, len(all_len))\n",
    "        t_succesive_explore_score /= max(1, len(all_len))\n",
    "        t_confirm_score /= max(1, len(all_len))\n",
    "        t_kl_trajectory_values /= max(1, len(all_len))\n",
    "        t_pred_no_gt /= max(1, len(all_len))\n",
    "        t_gt_no_pred /= max(1, len(all_len))\n",
    "        t_gt_pred_f1 /= max(1, len(all_len))\n",
    "\n",
    "        result[\"TVD\"] = tvd_sum\n",
    "        result[\"TrajScore\"] = kl_trajectory_values_sum\n",
    "        result[\"AUCTraj\"] = kl_trajectory_auc_sum\n",
    "        result[\"PlotData.x\"] = np.array(p)\n",
    "        result[\"PlotData.Exploration\"] = t_explore_score\n",
    "        result[\"PlotData.SuccessiveExploration\"] = t_succesive_explore_score\n",
    "        result[\"PlotData.Confirmation\"] = t_confirm_score\n",
    "        result[\"PlotData.Trajectory\"] = t_kl_trajectory_values\n",
    "        result[\"PlotData.SevF1\"] = t_gt_pred_f1\n",
    "        result[\"PlotData.SevPrecOut\"] = t_pred_no_gt\n",
    "        result[\"PlotData.SevRecIn\"] = t_gt_no_pred\n",
    "        \n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2545408e",
   "metadata": {},
   "source": [
    "compute_metrics 函数接受多个输入并根据这些输入计算各种评估指标。以下是该函数作用的概述： 该函数初始化一个名为 result 的空字典来存储计算的指标。 该函数计算有效时间步长的平均长度 (all_len) 并将其分配给结果 [\"IL\"]。 该函数计算不同截止点 (tres) 的地面真实准确度 (GTPA) 指标，并将它们分配给 result[\"GTPA\"]、result[\"GTPA@1\"]、result[\"GTPA@3\"] 和 result[ “GTPA@5”]。 该函数计算不同截止值 (tres) 的鉴别诊断召回率 (DDR)、鉴别诊断精度 (DDP) 和鉴别诊断 F1 分数 (DDF1) 指标，并将它们分配给 result[\"DDR\"], result[\"DDP\" ]，结果[“DDF1”]。 该函数计算不同截止点 (k) 的 DDR、DDP 和 DDF1 指标，并将它们分配给 result[\"DDR@k\"]、result[\"DDP@k\"] 和 result[\"DDF1@k\"] k值。 该函数根据预测疾病的严重程度计算疾病严重程度精度 (DSP)、疾病严重程度召回 (DSR) 和疾病严重程度 F1 分数 (DSF1) 指标，并将它们分配给 result[\"DSP\"], result[\" DSR\"] 和结果 [\"DSF1\"]。 该函数根据证据的存在计算当前证据率 (PER)、当前证据精度 (PEP) 和当前证据 F1 分数 (PEF1) 指标，并将它们分配给 result[\"PER\"], result[\"PEP\" ]，结果[“PEF1”]。 该函数根据症状的存在计算出现症状率 (PSR)、出现症状精度 (PSP) 和出现症状 F1 分数 (PSF1) 指标，并将它们分配给 result[\"PSR\"], result[\"PSP\" ]，结果[“PSF1”]。 该函数根据存在的病史 (ATCD) 计算当前 ATCD 率 (PAR)、当前 ATCD 精度 (PAP) 和当前 ATCD F1 分数 (PAF1) 指标，并将它们分配给结果 [\"PAR\"]，结果[“PAP”]，和结果[“PAF1”]。 该函数重塑预测的鉴别诊断概率，并根据这些概率计算确认分数、探索分数和连续探索分数。 该函数计算不同截止值 (tres) 的预测无地面真实情况、地面真实情况无预测和地面真实情况预测 F1 分数指标，并将它们分配给 t_pred_no_gt、t_gt_no_pred 和 t_gt_pred_f1。 该函数计算最小和最大概率之间的总变异距离 (TVD)，并将其分配给 result[\"TVD\"]。 该函数根据确认和探索分数计算轨迹分数和轨迹 AUC（曲线下面积），并将它们分配给 result[\"TrajScore\"] 和 result[\"AUCTraj\"]。 该函数创建一个百分比值 (p) 数组，并使用 get_average_state_from_percent 函数计算每个指标在不同百分比截止点的平均状态。计算出的平均状态被分配给结果字典中的相应键。 最后，该函数返回包含所有计算指标的结果字典。 请注意，一些函数如 compute_f1、kl_confirm_score、kl_explore_score、kl_trajectory_score、kl_trajectory_auc 和 get_average_state_from_percent 假定在代码的其他地方定义。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
